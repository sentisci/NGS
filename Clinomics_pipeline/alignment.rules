##i1. Pre-processing QC statistics
##1a. FastQC
rule fastqc:
	input:
                fastq = lambda wildcards: config['units'][wildcards.unit]
	output:
                "{base}/qc/fastqc/{unit}_R1_fastqc.html",
                "{base}/qc/fastqc/{unit}_R2_fastqc.html",
	version:
		config['fastqc']        
	params:
		rulename	="Fastqc",
		batch		=config['fastqc_clust']
	log:
		"log_error/{unit}.fastqc.log"
	shell: """
	
	module load fastqc/{version}
	
	mkdir /projects/scratch/${{PBS_JOBID}}
	
        fastqc -t 6 -o {wildcards.base}/qc/fastqc/ -d /projects/scratch/${{PBS_JOBID}} {input.fastq[0]} 2>> {log}
        fastqc -t 6 -o {wildcards.base}/qc/fastqc/ -d /projects/scratch/${{PBS_JOBID}} {input.fastq[1]} 2>> {log}
	
	bn_R1_lib=`basename {input.fastq[0]}`; bn_R1_lib_fastqc_html="${{bn_R1_lib/.fastq.gz/_fastqc.html}}"; bn_R1_lib_fastqc_zip="${{bn_R1_lib/.fastq.gz/_fastqc.zip}}"
        bn_R2_lib=`basename {input.fastq[1]}`; bn_R2_lib_fastqc_html="${{bn_R2_lib/.fastq.gz/_fastqc.html}}"; bn_R2_lib_fastqc_zip="${{bn_R2_lib/.fastq.gz/_fastqc.zip}}"

        bn_R1_unit_html=`basename {wildcards.unit}_R1_fastqc.html` ; bn_R1_unit_zip=`basename {wildcards.unit}_R1_fastqc.zip`
        bn_R2_unit_html=`basename {wildcards.unit}_R2_fastqc.html` ; bn_R2_unit_zip=`basename {wildcards.unit}_R2_fastqc.zip`
	
        #make links to escape the rerun of rule
        ln -s `pwd`/{wildcards.base}/qc/fastqc/${{bn_R1_lib_fastqc_html}}    `pwd`/{wildcards.base}/qc/fastqc/${{bn_R1_unit_html}}
        ln -s `pwd`/{wildcards.base}/qc/fastqc/${{bn_R1_lib_fastqc_zip}}    `pwd`/{wildcards.base}/qc/fastqc/${{bn_R1_unit_zip}}
        ln -s `pwd`/{wildcards.base}/qc/fastqc/${{bn_R2_lib_fastqc_html}}    `pwd`/{wildcards.base}/qc/fastqc/${{bn_R2_unit_html}}
        ln -s `pwd`/{wildcards.base}/qc/fastqc/${{bn_R2_lib_fastqc_zip}}    `pwd`/{wildcards.base}/qc/fastqc/${{bn_R2_unit_zip}}

		"""
##2. Post-processing QC statistics
##2a. Samtools Flagstat	
rule flagstat:
	input:
		bam="{base}/bam/{sample}.{aligner}.final.bam",
		bam_bai="{base}/bam/{sample}.{aligner}.final.bai"
	output:
		"{base}/qc/{sample}.{aligner}.final.bam.flagstat"
	version:
		config['samtools']
	params:
		rulename	= "flagstat",
		batch		= config['flagstat_clust']
	log:	
		"log_error/{sample}.{aligner}.final.bam.flagstat.log"
	shell:	"""
	
	module load samtools/{version}
	
	samtools flagstat {input.bam} > {output} 2> {log}
		"""

##2b. Qualimap - bamqc
rule bamqc:
        input:
                bam="{base}/bam/{sample}.{aligner}.final.bam",
                bam_bai="{base}/bam/{sample}.{aligner}.final.bai"
        output:
                "{base}/qc/bamqc/{sample}.{aligner}.final.bam.qualimapReport.html"
        version:
                config['qualimap']
        params:
                rulename        = "BamQc",
		target_intervals_gff=lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]].replace('.bed', '.gff'),
                batch           = config['bamqc_clust']
        log:	"log_error/{sample}.final.bam.qualimapReport.log"
        shell:  """

        module load qualimap/{version}
	module load java/jre1.7.0_71
        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`

        qualimap bamqc -c -bam {input.bam} -outdir {wildcards.base}/qc/bamqc -gff {params.target_intervals_gff} -nt 16 --java-mem-size=${{mem}}G >{log} 2>&1
        if [ -f "{wildcards.base}/qc/bamqc/qualimapReport.html" ]; then
		mv {wildcards.base}/qc/bamqc/qualimapReport.html {output}
	fi
	"""

##2c. IGVTools -  Tiled data file(.tdf) 
rule bamtdf:
	input:
                bam="{base}/bam/{sample}.{aligner}.final.bam",
                bam_bai="{base}/bam/{sample}.{aligner}.final.bai"
	output:
        	"{base}/bam/{sample}.{aligner}.final.tdf"
	version:	
		config['igvtools']
	params:
        	rulename        = "bamtdf",
		genome		= config['reference_name'].replace("human_g1k_v37", "1kg_v37").replace("ucsc.hg19", "hg19"),
		batch           = config['igvtools_clust']
	log:    "log_error/{sample}.final.bam.tdf.log"
	shell:  """
	
	module load IGVTools/{version}
	
	igvtools count {input.bam} {output} {params.genome} > {log} 2>&1

		"""	

#2d.	BedTools - Read Depth
rule readDepth:
	input:
                bam="{base}/bam/{sample}.{aligner}.final.bam",
                bam_bai="{base}/bam/{sample}.{aligner}.final.bai"
	output:
                "{base}/qc/{sample}.{aligner}.final.bam.depth_per_base"
	version:
		config['bedtools']
	params:
		rulename	= "readDepth",
		target_intervals=lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]],
		read_depth_script=config['readDepth_script'],
		batch		= config['readDepth_clust']
	log:	"log_error/{sample}.final.bam.depth.log"
	shell: """
	
	module load bedtools/{version}
	module load R
		
	{params.read_depth_script} {input.bam} {params.target_intervals} {wildcards.base}/qc/ > {log} 2>&1	
		"""

## mpileup snpcalling
rule mpileupGenotyping:
	input:
		bam="{base}/bam/{sample}.{aligner}.final.bam",
		bai="{base}/bam/{sample}.{aligner}.final.bai",
	output:
		vcf="{base}/{sample}/calls/Genotype/{sample}.{aligner}.samtools.vcf",
		gt="{base}/qc/Genotype/{sample}.{aligner}.final.bam.gt"
	params:
		rulename		="mpileupGenotyping",
		reference		=config['reference_fasta'][config['reference_name']],
		genotyping_sites	=config['genotyping_sites'],
		vcf2genotype_script	=config['vcf2genotype_script'],
		batch			=config['mpileupGenotyping_clust']
	version:
		config['samtools']
	log:
		"log_error/{sample}.final.bam.gt.log"
	shell: """
        
	module load samtools/{version} 

        samtools mpileup -u -C50  -f {params.reference} -l {params.genotyping_sites} {input.bam} | bcftools view -gc - >{output.vcf} 2>{log}
	perl {params.vcf2genotype_script} {output.vcf} > {output.gt} 2>> {log}

        cp -f {output.gt} /projects/Clinomics/Tools/Genotyping/GenotypeFiles/{wildcards.sample}.gt 2>>{log}
        """

rule genotyping:
	input:
		Genotype_files
	output:	
		temp("Genotyping_done")
	params:
		rulename	= "genotyping",
		genotype_script	= config['genotype_script'],
		batch		= config['genotype_clust']
	shell: """
	
	cd /projects/Clinomics/Tools/Genotyping/
	sh {params.genotype_script} 
	touch `basename {output}`
	chmod 770 * 2> /dev/null

	touch Genotyping_done
	"""

rule subject_genotyping:
	input:
		gtFiles=lambda wildcards: SUB_GT[wildcards.subject],

	output:
		"SUBJECT/{subject}/"+NOW+"/Genotype/{subject}.genotyping.txt",
	params:
		rulename = "subject_genotyping",
		mail_script=NGS_PIPELINE + "/scripts/tsv2html.sh",
		score_script=NGS_PIPELINE + "/scripts/scoreGenotyes.pl",
		batch    = config["sample_genotyping_clust"]
	shell: """
	
	mkdir -p SUBJECT/{wildcards.subject}/"+NOW+"/Genotype/GT
	mkdir -p SUBJECT/{wildcards.subject}/"+NOW+"/Genotype/RATIO
	cp {input.gtFiles} SUBJECT/{wildcards.subject}/"+NOW+"/Genotype/GT/
	echo Sample > SUBJECT/{wildcards.subject}/"+NOW+"/Genotype/RATIO/FirstColumn

	for file in SUBJECT/{wildcards.subject}/"+NOW+"/Genotype/GT/*
	do
        	sample=`basename ${{file}} .gt`
        	echo ${{sample}} >>{wildcards.subject}/qc/RATIO/FirstColumn
        	echo ${{sample}} >>{wildcards.subject}/qc/RATIO/${{sample}}.ratio
        	for file2 in {wildcards.subject}/qc/GT/*
        	do
			perl {input.score} ${{file}} ${{file2}} >>{wildcards.subject}/qc/RATIO/${{sample}}.ratio
        	done
	done
	paste {wildcards.subject}/qc/RATIO/FirstColumn {wildcards.subject}/qc/RATIO/*.ratio >{wildcards.subject}/qc/{wildcards.subject}.genotyping.txt
	rm -rf {wildcards.subject}/qc/GT/ {wildcards.subject}/qc/RATIO/
	sed -i 's/Sample_//g' {wildcards.subject}/qc/{wildcards.subject}.genotyping.txt
	sed -i 's/.bwa//g' {wildcards.subject}/qc/{wildcards.subject}.genotyping.txt
	sed -i 's/.star//g' {wildcards.subject}/qc/{wildcards.subject}.genotyping.txt
	sh {input.mail} --name {wildcards.subject} --head {wildcards.subject}/qc/{wildcards.subject}.genotyping.txt | mutt -e "my_hdr Content-Type: text/html" sivasish.sindiri@nih.gov -c sivasish.sindiri@nih.gov -s 'Genotyping Result on {wildcards.subject}'
	"""

#2d.	In-house script - failedExons
rule  failedExons:
	input:
		depth_file="{base}/qc/{sample}.{aligner}.final.bam.depth_per_base"
	output:
		failed_exon_stats="{base}/qc/{sample}.{aligner}.final.bam.failExons",
		failed_gene_stats="{base}/qc/{sample}.{aligner}.final.bam.failGenes"
	params:
		rulename	=	"failedExons",
		threshold	=	lambda wildcards: config['failed_exon_params'][config['sample_captures'][wildcards.sample][0]][config['sample_TN'][wildcards.sample][0]],
		failed_Exons_script=	config['failedExons_script'],
		batch		=	config['failedExons_clust']
	log:    "log_error/{sample}.failed.exon.log"
	shell:	"""

	perl {params.failed_Exons_script} {input.depth_file} {params.threshold} {output.failed_exon_stats} {output.failed_gene_stats}
		"""


#2e.	Samtools - Target Interval Depth	
rule targetIntervals:
        input:
                bam="{base}/bam/{sample}.{aligner}.final.bam",
                bam_bai="{base}/bam/{sample}.{aligner}.final.bai"
        output:
	        bam_probe_intervals  = temp("{base}/qc/{sample}.{aligner}.final.bam.probe.intervals"),
	        bam_target_intervals = temp("{base}/qc/{sample}.{aligner}.final.bam.target.intervals")
	version:
                config['samtools']
	params:
		rulename = "targetIntervals",
	        target_intervals=lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]],
	        probe_intervals=lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]].replace(".target.", ".design."),
		batch	= config['targetIntervals_clust']
	log:	"log_error/{sample}.targetIntervals.log"
	shell:	"""
		
	module load samtools/{version}
	cat <(samtools view -H {input.bam}) <(gawk '{{print $1 "\t" $2+1 "\t" $3 "\t+\tinterval_" NR}}' {params.probe_intervals} )> {output.bam_probe_intervals} 2>> {log}

	cat <(samtools view -H {input.bam}) <(gawk '{{print $1 "\t" $2+1 "\t" $3 "\t+\tinterval_" NR}}' {params.target_intervals} )> {output.bam_target_intervals} 2>> {log}

		"""

#2f.	Picard - CalculateHsMetrics
rule hsMetrics:
	input:
        	bam="{base}/bam/{sample}.{aligner}.final.bam",
        	bam_probe_intervals = "{base}/qc/{sample}.{aligner}.final.bam.probe.intervals",
        	bam_target_intervals = "{base}/qc/{sample}.{aligner}.final.bam.target.intervals",
	output:
		"{base}/qc/{sample}.{aligner}.final.bam.hsmetrics"
	version: 
		config['picard']
	params:
        	rulename = "hsMetrics",
        	reference = config['reference_fasta'][config['reference_name']],
		batch	= config['hsMetrics_clust']
	log:	"log_error/{sample}.final.bam.hsmetrics.log"
	shell: """

	module load picard/{version}
	module load java/jre1.7.0_71
        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`

	java -Xmx${{mem}}g -Djava.io.tmpdir=/projects/scratch/${{PBS_JOBID}} -jar /apps/picard/{version}/picard.jar CalculateHsMetrics BAIT_INTERVALS={input.bam_probe_intervals} TARGET_INTERVALS={input.bam_target_intervals} INPUT={input.bam} OUTPUT={output} METRIC_ACCUMULATION_LEVEL=ALL_READS REFERENCE_SEQUENCE={params.reference} QUIET=true  VALIDATION_STRINGENCY=SILENT > {log} 2>&1
		
	"""

#2g.	Samtools - Hotspot genes coverage
rule hotspotCov:
	input:
        	bam="{base}/bam/{sample}.{aligner}.final.bam"
	output:
        	"{base}/qc/{sample}.{aligner}.final.bam.hotspot.depth"
	version:
		config['samtools']
	params:
        	rulename		= "hotspotCov",
        	hotspot_intervals 	= config['hotspot_intervals'],
		bedtools_version	= config['bedtools'],
        	batch			= config['hotspotCov_clust']
	log:	
		"log_error/{sample}.final.bam.hotspot.log"
	shell: """
	
	module load samtools/{version} 
	module load bedtools/{params.bedtools_version}
	
	samtools view -hF 0x400 -q 30 {input} | samtools view -ShF 0x4 - | samtools view -SuF 0x200 - | bedtools coverage -abam - -b {params.hotspot_intervals} > {output}
	
		"""

#2h.	Python Consolidated QC script
rule consolidated_QC:
	input:
		bam="{base}/bam/{sample}.{aligner}.final.bam"
	output:
		consolidated_QC_output="{base}/qc/{sample}.{aligner}.final.bam.consolidated_QC"
	version:
		config['samtools']
	params:
		rulename		=	"consolidated_QC",
		target_intervals	=	lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]],
		bedtools_version	=	config['bedtools'],
		consolidated_QC_script	=	config['consolidated_QC_script'],	
		batch   		=	config['consolidated_QC_clust']
	log:
		"log_error/{sample}.consolidated_QC.log"
	shell:	"""
	
	module unload python/3.4.3
	module load python/2.7.9
	module load samtools/{version}
	module load bedtools/{params.bedtools_version}
	
	mkdir /projects/scratch/${{PBS_JOBID}}
	
	python {params.consolidated_QC_script} {input.bam} {params.target_intervals} /projects/scratch/${{PBS_JOBID}}  > {output.consolidated_QC_output}		
		"""

##3. Alignment Algorithms
##3a. BWA mem 
rule bwamem_map:
	input:
		index = lambda wildcards: config['align_algo_index']['bwa']+config['bwa']+"/"+config['reference_name']+".pac",
		fastq = lambda wildcards: config['units'][wildcards.unit]
	output:
		bam="TEMP/{unit}.bwamem.bam",
		bai="TEMP/{unit}.bwamem.bam.bai"
	version: 
		config['bwa']
	params:
        	rulename 		= "bwamem_map",
		sample			= lambda wildcards:     UNIT_TO_SAMPLE[wildcards.unit],
        	library 		= lambda wildcards:	UNIT_TO_LIBRARY[wildcards.unit],
        	platform 		= config['platform'],
        	bwa_index 		= lambda wildcards: config['align_algo_index']['bwa']+config['bwa']+"/"+config['reference_name'],
		adapters		= config['adapters'],
		samtools_version	= config['samtools'],
		ea_utils_version	= config['ea_utils'],
		fastqc_version		= config['fastqc'],
        	batch			= config['alignment_clust']['bwa']
	log:	"log_error/{unit}.bwamap.log"
    	shell:	"""
	
	module load bwa/{version}
	module load fastqc/{params.fastqc_version}
	module load samtools/{params.samtools_version}
	module load ea-utils/{params.ea_utils_version}

	mkdir /projects/scratch/${{PBS_JOBID}}
	
	R1=/projects/scratch/${{PBS_JOBID}}/`basename {input.fastq[0]}`
	R2=/projects/scratch/${{PBS_JOBID}}/`basename {input.fastq[1]}`

	mcf_log={wildcards.unit}.mcf_log	

	fastq-mcf -C 1000000 -q 2 -p 10 -u -x 20 -o $R1 -o $R2 {params.adapters} <(gunzip -c {input.fastq[0]}) <(gunzip -c {input.fastq[1]}) > /projects/scratch/${{PBS_JOBID}}/${{mcf_log}} 2>&1
	bwa mem -M -t 16 -R "@RG\tID:{wildcards.unit}\tSM:{params.sample}\tLB:{params.library}\tPL:Illumina" {params.bwa_index} $R1 $R2 2> {log}| samtools view -Sbh - |samtools sort -m 30000000000 - TEMP/{wildcards.unit}.bwamem
	
	#indexing BAM file
	samtools index TEMP/{wildcards.unit}.bwamem.bam
		"""

##3b.	Novoalign Alignment
rule novoalign:
	input:
		index = lambda wildcards: config['align_algo_index']['novoalign'],
		fastq = lambda wildcards: config['units'][wildcards.unit]
	output:
		bam="TEMP/{unit}.novoalign.bam",
		bai="TEMP/{unit}.novoalign.bam.bai"
	version: 
		config["novocraft"]
	params:
		rulename  		= "novoalign",
		sample                  = lambda wildcards:     UNIT_TO_SAMPLE[wildcards.unit],
		library                 = lambda wildcards:     UNIT_TO_LIBRARY[wildcards.unit],
		platform  		= config["platform"],
		samtools_version        = config['samtools'],
		batch     		= config["novoalign_clust"]
	log:    
		"log_error/{unit}.novoalign.log"
	shell: """
	
	module load novocraft/{version}
	module load samtools/{params.samtools_version}

	mkdir /projects/scratch/${{PBS_JOBID}}

	`cat /cm/local/apps/torque/var/spool/aux/${{PBS_JOBID}} | sort | uniq > /projects/scratch/${{PBS_JOBID}}/hosts.txt`	
	
	mpiexec -f /projects/scratch/${{PBS_JOBID}}/hosts.txt -np 3 novoalignMPI -d {input.index} -f {input.fastq[0]} {input.fastq[1]} -a AGATCGGAAGAGCGGTTCAGCAGGAATGCCGAG AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGTA -c 30 -e 100 -F STDFQ --hlimit 7 -i 200 100 -l 30 -o SAM  \"@RG\\tID:{params.sample}\\tSM:{params.sample}\\tLB:{params.sample}\\tPL:{params.platform}\" -p 5,2 -t 250  2>{log} | samtools view -Sbh - | samtools sort -m 30000000000 - TEMP/{wildcards.unit}.novoalign

	samtools index {output.bam}
	"""

##4.	Post-Processing of BAM files
#4a.	PICARD - MarkDuplicate
rule markdups:
	input:	
		bam="TEMP/{unit}.{aligner}.bam",
		bai="TEMP/{unit}.{aligner}.bam.bai"
	output:
		bam="TEMP/{unit}.{aligner}.md.bam",
		bai="TEMP/{unit}.{aligner}.md.bam.bai",
		metrics="TEMP/{unit}.{aligner}.bam.dupmetrics"
	version:
		config['picard']
	params:
		rulename	= "markdups",
		picard_version	= config['picard'],
		samtools_version= config['samtools'],
		batch		= config['picard_clust']['picard_md']
	log:	
		"log_error/{unit}.md.bam.log"
	shell:	"""
	
	module load picard/{version}
	module load samtools/{params.samtools_version}
	mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`

	mkdir /projects/scratch/${{PBS_JOBID}}
	pwd=`pwd`
	echo $pwd	
	
	java -Xmx${{mem}}g -Djava.io.tmpdir=/projects/scratch/${{PBS_JOBID}} -jar /apps/picard/{version}/picard.jar MarkDuplicates AS=true M={output.metrics} O={output.bam} I={input.bam} REMOVE_DUPLICATES=false VALIDATION_STRINGENCY=SILENT 2> {log}
	
	samtools index {output.bam}
		"""

#4b.	PICARD - MergeSamFiles
rule mergeBams:
	input:
		bam=lambda wildcards:  expand("TEMP/{unit}.{aligner}.md.bam", unit = SAMPLE_TO_UNIT[wildcards.sample],aligner= config['aligner'][wildcards.aligner]),
	output:
        	bam="TEMP/{sample}.{aligner}.merge.bam",
		bai="TEMP/{sample}.{aligner}.merge.bam.bai"
	version:
		config['picard']
	params:
		rulename	= "mergeBams",
		samtools_version= config['samtools'],
		batch		= config['picard_clust']['picard_md']
	log:	
		"log_error/{sample}.merge.bam.log"
	run:
		inputBams = ' '.join(['I={bam}'.format(bam=bam) for bam in input])
		print(inputBams)
		shell("""
			module load picard/{version}
			module load samtools/{params.samtools_version}
		        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`

			java -Xmx${{mem}}g -Djava.io.tmpdir=/projects/scratch/${{PBS_JOBID}} -jar /apps/picard/{version}/picard.jar  MergeSamFiles AS=true USE_THREADING=true VALIDATION_STRINGENCY=SILENT {inputBams} O={output.bam} 2>> {log}
			samtools index {output.bam}
		""")

##5.	Indel and Variant Calling
##5a.	bam2mpg - Most probable genotype program for predicting variants and genotypes from alignments of short reads in BAM format.
rule Bam2MPG:
	input:
		bam=lambda wildcards:  expand("TEMP/{sample}.{aligner}.merge.bam", sample=wildcards.sample, aligner= config['aligner'][wildcards.aligner]),
		bai=lambda wildcards:  expand("TEMP/{sample}.{aligner}.merge.bam.bai", sample=wildcards.sample, aligner= config['aligner'][wildcards.aligner])
	output:
		vcf="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/bam2mpg/{sample}.{aligner}.bam2mpg.raw.vcf"
	version: 
		config["bam2mpg"]
	params:
		rulename		= "bam2mpg",
		reference		= config['reference_fasta'][config['reference_name']],
		target_intervals	=lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]],
		samtools_version  	= config["samtools"],
		vcftools_version  	= config["vcftools"],
		batch			= config["bam2mpg_clust"]
	log: 
		"log_error/{sample}.bam2mpg.log"
	shell: """

	module load bam2mpg/{version}
	module load vcftools/{params.vcftools_version}
	module load samtools/{params.samtools_version}

	mkdir /projects/scratch/${{PBS_JOBID}}
	temp_dir="/projects/scratch/${{PBS_JOBID}}"

	##Run Bam2mpg for each chromosome
	for CHR in `seq 1 22` X Y;
	do
		bam2mpg --qual_filter 20 -bam_filter '-q31' --region chr${{CHR}} --only_nonref --snv_vcf ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.snps.vcf --div_vcf ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.indel.vcf {params.reference} {input.bam} &
	done
	wait

	##Sort, bzip and index the above files
	for CHR in `seq 1 22` X Y;
	do
		cat ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.snps.vcf | vcf-sort > ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.snps.tmp.vcf
		bgzip ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.snps.tmp.vcf
		tabix -p -f vcf ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.snps.tmp.vcf.gz
		
		cat ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.indel.vcf | vcf-sort > ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.indel.tmp.vcf
		bgzip ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.indel.tmp.vcf
		tabix -p -f vcf ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.indel.tmp.vcf.gz

		rm ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.snps.vcf ${{temp_dir}}/chr${{CHR}}{wildcards.sample}.indel.vcf
	done

	echo "Combine chr level vcf files"
	vcf-concat ${{temp_dir}}/chr*{wildcards.sample}.*.vcf.gz > ${{temp_dir}}/{wildcards.sample}.vcf
	
	echo "Restrict to Bed file"
	vcftools --vcf ${{temp_dir}}/{wildcards.sample}.vcf --bed {params.target_intervals} --out {wildcards.sample} --recode --keep-INFO-all

	sed -e 's/SAMPLE/{wildcards.sample}/g' {wildcards.sample}.recode.vcf | vcf-sort > SUBJECT/{wildcards.subject}/{NOW}/{wildcards.sample}/{wildcards.ref}.{wildcards.aligner}/bam2mpg/{wildcards.sample}.{wildcards.aligner}.bam2mpg.raw.vcf

	rm -rf {wildcards.sample}.recode.vcf

	mkdir -p SUBJECT/{wildcards.subject}/{NOW}/{wildcards.sample}/{wildcards.ref}.{wildcards.aligner}/bam	
	cp {input.bam} SUBJECT/{wildcards.subject}/{NOW}/{wildcards.sample}/{wildcards.ref}.{wildcards.aligner}/bam/{wildcards.sample}.{wildcards.aligner}.final.bam
	"""

##5b.	GATK pipeline - The GATK Best Practices workflow. They enable discovery of SNPs and small indels (no size limit in theory but adjustments may be required to call indels > 50 bp) in DNA
rule gatk_realigner_target_creator:
	input:
		bam="TEMP/{sample}.{aligner}.merge.bam",
                phase1=config['resources']['knownIndels']['phase1'],
                mills=config['resources']['knownIndels']['mills'],
                reference=config['reference_fasta'][config['reference_name']]
	output:
		ri="TEMP/{sample}.{aligner}.realignment.intervals"
	version:
                config['gatk']
	params:
                rulename        = "gatk_realigner_target_creator",
                batch           = config['gatk_clust']
	log:
                "log_error/{sample}.gatk.realigner_target.log"
	shell:  """

        module load GATK/{version}
        module load java/jre1.7.0_71
        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`

        java -Xmx${{mem}}g -Djava.io.tmpdir=/projects/scratch/${{PBS_JOBID}} -jar /apps/GATK/{version}/GenomeAnalysisTK.jar -T RealignerTargetCreator -R {input.reference} -known {input.phase1} -known {input.mills} -I {input.bam} -o {output.ri} -nt 8 >{log} 2>&1
		"""		

rule gatk_IndelRealigner:
	input:
		bam="TEMP/{sample}.{aligner}.merge.bam",	
		ri="TEMP/{sample}.{aligner}.realignment.intervals",
                phase1=config['resources']['knownIndels']['phase1'],
                mills=config['resources']['knownIndels']['mills'],
                reference=config['reference_fasta'][config['reference_name']]
	output:
		lr_bam="TEMP/{sample}.{aligner}.lr.bam"
	version:
                config['gatk']
	params:
                rulename        = "gatk_IndelRealigner",
                batch           = config['gatk_clust']
	log:
                "log_error/{sample}.gatk.IndelRealigner.log"
	shell:  """

        module load GATK/{version}
        module load java/jre1.7.0_71
        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`
	
	java -Xmx${{mem}}g -Djava.io.tmpdir=/projects/scratch/${{PBS_JOBID}} -jar /apps/GATK/{version}/GenomeAnalysisTK.jar -T IndelRealigner -R {input.reference}  -known {input.phase1} -known {input.mills} -I {input.bam} --targetIntervals {input.ri} -o {output.lr_bam} >>{log} 2>&1	

		"""

rule gatk_BaseRecalibrator:
	input:
		lr_bam="TEMP/{sample}.{aligner}.lr.bam",
                phase1=config['resources']['knownIndels']['phase1'],
                mills=config['resources']['knownIndels']['mills'],
                reference=config['reference_fasta'][config['reference_name']]
	output:
		rmat="TEMP/{sample}.{aligner}.recalibration.matrix.txt"
	version:
                config['gatk']
	params:
                rulename        = "gatk_BaseRecalibrator",
                batch           = config['gatk_clust']
	log:
                "log_error/{sample}.gatk.BaseRecalibrator.log"
	shell:  """

        module load GATK/{version}
        module load java/jre1.7.0_71
        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`

	java -Xmx${{mem}}g -Djava.io.tmpdir=/projects/scratch/${{PBS_JOBID}} -jar /apps/GATK/{version}/GenomeAnalysisTK.jar -T BaseRecalibrator -R {input.reference} -knownSites {input.phase1} -knownSites {input.mills} -I {input.lr_bam} -o {output.rmat} >>{log} 2>&1	
		"""

rule gatk_PrintReads:
	input:
                lr_bam="TEMP/{sample}.{aligner}.lr.bam",
		rmat="TEMP/{sample}.{aligner}.recalibration.matrix.txt",
                reference=config['reference_fasta'][config['reference_name']]
	output:
                bam="{base}/bam/{sample}.{aligner}.final.bam",
                index="{base}/bam/{sample}.{aligner}.final.bai"
	version:
                config['gatk']
	params:
                rulename        = "gatk_PrintReads",
                batch           = config['gatk_clust']
	log:
                "log_error/{sample}.gatk.PrintReads.log"
	shell:  """

        module load GATK/{version}
        module load java/jre1.7.0_71
        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`

	java -Xmx${{mem}}g -Djava.io.tmpdir=/projects/scratch/${{PBS_JOBID}} -jar /apps/GATK/{version}/GenomeAnalysisTK.jar -T PrintReads -R {input.reference} -I {input.lr_bam} -o {output.bam} -BQSR {input.rmat} >>{log} 2>&1				
		"""

##6.	Germline and Somatic Variant Callers
#6a.	Germline Caller : GATK - HaplotypeCaller
rule HaplotypeCaller:
	input:
		bam="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/bam/{sample}.{aligner}.final.bam",
		bai="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/bam/{sample}.{aligner}.final.bai"
	output:	
		vcf="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/haplotypecaller/{sample}.{aligner}.haplotypecaller.raw.vcf"
	version:
		config['gatk']
	params:
		rulename	="HaplotypeCaller",
		reference	=config['reference_fasta'][config['reference_name']],
		dbsnp		=config["resources"]["dbsnp"],
		target_intervals=lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]],
		batch           = config['gatk_clust']
		
	log :	
		"log_error/{sample}.haplotypecaller.raw.vcf.log"
	shell: """
	module load GATK/{version}
	module load java/jre1.7.0_71
        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`
	
	mkdir /projects/scratch/${{PBS_JOBID}}

	gawk '{{print $1 "\t" $2-1 "\t" $3}}' {params.target_intervals} > /projects/scratch/${{PBS_JOBID}}/target_intervals.bed
	java -Xmx${{mem}}g -Djava.io.tmpdir=/projects/scratch/${{PBS_JOBID}} -jar /apps/GATK/{version}/GenomeAnalysisTK.jar  -T HaplotypeCaller -R {params.reference} -I {input.bam} -L /projects/scratch/${{PBS_JOBID}}/target_intervals.bed -o {output.vcf} --dbsnp {params.dbsnp} -mbq 20 -mmq 30 -log {log}
	
	       """

#6b.	Germline Caller : freebayes -	Bayesian haplotype-based polymorphism discovery and genotyping.
rule freebayes:
        input:
                bam="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/bam/{sample}.{aligner}.final.bam",
                bai="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/bam/{sample}.{aligner}.final.bai"
        output:
                vcf="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/freebayes/{sample}.{aligner}.freebayes.raw.vcf"
        version:
                config['freebayes']
        params:
                rulename        ="Freebayes",
                reference       =config['reference_fasta'][config['reference_name']],
                dbsnp           =config["resources"]["dbsnp"],
                target_intervals=lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]],
	        vcftools_version=config['vcftools'],
		batch           = config['freebayes_clust']

        log :
                "log_error/{sample}.freeayes.raw.vcf.log"
        shell: """
        module load freebayes/{version}
	module load vcftools/{params.vcftools_version}
	
	mkdir /projects/scratch/${{PBS_JOBID}}
       
	gawk '{{print $1 "\t" $2-1 "\t" $3}}' {params.target_intervals} > /projects/scratch/${{PBS_JOBID}}/target_intervals.bed
        freebayes -f {params.reference} --haplotype-length 50 -b {input.bam} -v {output} > {log} 2>&1

	vcftools --vcf {output} --bed /projects/scratch/${{PBS_JOBID}}/target_intervals.bed --out {output} --recode --keep-INFO-all
	mv {output}.recode.vcf {output.vcf}
               """

#6C.	Germline Caller : platypus -  A Haplotype-Based Variant Caller.
rule platypus:
        input:
                bam="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/bam/{sample}.{aligner}.final.bam",
                bai="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/bam/{sample}.{aligner}.final.bai"
        output:
                vcf="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/platypus/{sample}.{aligner}.platypus.raw.vcf"
        version:
                config['platypus']
        params:
                rulename        ="Platypus",
                reference       =config['reference_fasta'][config['reference_name']],
                dbsnp           =config["resources"]["dbsnp"],
                target_intervals=lambda wildcards: config['target_intervals'][config['sample_captures'][wildcards.sample][0]],
                batch           = config['platypus_clust']

        log :
                "log_error/{sample}.platypus.raw.vcf.log"
        shell: """
        module load Platypus/{version}

	mkdir /projects/scratch/${{PBS_JOBID}}
	
	gawk '{{print $1 ":" $2 "-" $3}}' {params.target_intervals} > /projects/scratch/${{PBS_JOBID}}/target_intervals.txt
	platypus callVariants --nCPU=16 --bufferSize=1000000 --maxReads=100000000 --bamFiles={input.bam} --regions=/projects/scratch/${{PBS_JOBID}}/target_intervals.txt --output={output.vcf} --refFile={params.reference} --logFileName={log}
	sed -i 's/.bwamem.final//g' {output.vcf}
               """

#6D.	Somatic Callers	:	MuTect - Bayesian classifier to detect somatic mutations from sequencing data of matched tumor–normal samples  .
rule mutect:
        input:
                lambda wildcards: somaticPairs[wildcards.somaticPair]
        output:
	        call_stats="{anything}/mutect/{somaticPair}.{aligner}.mutect.call_stats.txt",
	        coverage="{anything}/mutect/{somaticPair}.{aligner}.mutect.coverage.wig.txt",
	        vcf="{anything}/mutect/{somaticPair}.{aligner}.mutect.raw.vcf"
        version:
                config['MuTect']
        params:
                rulename        ="MuTect",
                reference       =config['reference_fasta'][config['reference_name']],
                dbsnp           =config["resources"]["dbsnp"],
		cosmic		=config["resources"]["cosmic"],
                target_intervals=lambda wildcards: config['target_intervals'][PairsCapture[wildcards.somaticPair][0]],
                batch           = config['MuTect_clust']

        log :
                "log_error/{somaticPair}.mutect.raw.vcf.log"
	shell: """
        
	module load muTect/{version}
	module load java/jre1.7.0_71
        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`

	mkdir /projects/scratch/${{PBS_JOBID}}	
	
	gawk '{{print $1 "\t" $2-1 "\t" $3}}' {params.target_intervals} > /projects/scratch/${{PBS_JOBID}}/target_intervals.bed
	java -Xmx${{mem}}g -Djava.io.tmpdir=/projects/scratch/${{PBS_JOBID}} -jar /apps/muTect/{version}/mutect-{version}.jar -T MuTect --reference_sequence {params.reference} --cosmic {params.cosmic} --dbsnp {params.dbsnp} --input_file:normal {input[0]} --input_file:tumor {input[2]} --out {output.call_stats} --coverage_file {output.coverage} --intervals /projects/scratch/${{PBS_JOBID}}/target_intervals.bed --vcf  {output.vcf} --max_alt_allele_in_normal_fraction 0.05 --max_alt_alleles_in_normal_count 4 > {log} 2>&1
	"""

#6D.	Somatic Callers	:	Strelka - Bayesian classifier to detect somatic mutations from sequencing data of matched tumor–normal samples	.
rule strelka:
        input:
                lambda wildcards: somaticPairs[wildcards.sample]
        output:
	        snv_vcf="{anything}/strelka/{sample}.{aligner}.strelka.snvs.raw.vcf",
	        indel_vcf="{anything}/strelka/{sample}.{aligner}.strelka.indels.raw.vcf"
	version:
                config['Strelka']
	params:
        	rulename        ="Strelka",
        	reference       =config['reference_fasta'][config['reference_name']],
		strelka_config  =config['strelka_config'],
        	target_intervals=lambda wildcards: config['target_intervals'][PairsCapture[wildcards.sample][0]],
		vcftools_version=config['vcftools'],
        	batch           = config['Strelka_clust']
	log :
                "log_error/{sample}.strelka.raw.vcf.log"
	shell: """
	module load strelka/{version}
	module load vcftools/{params.vcftools_version}
	module load samtools	
	
	mkdir /projects/scratch/${{PBS_JOBID}}	
	
	configureStrelkaWorkflow.pl --normal={input[0]} --tumor={input[2]} --ref={params.reference} --config={params.strelka_config} --output-dir=/projects/scratch/${{PBS_JOBID}}/strelka/ > {log} 2>&1
	
	make -j 16 -f /projects/scratch/${{PBS_JOBID}}/strelka/Makefile 2>> {log}

	vcftools --vcf /projects/scratch/${{PBS_JOBID}}/strelka/results/passed.somatic.snvs.vcf --bed {params.target_intervals} --out {output.snv_vcf} --recode --keep-INFO-all
	mv {output.snv_vcf}.recode.vcf {output.snv_vcf}

	vcftools --vcf /projects/scratch/${{PBS_JOBID}}/strelka/results/passed.somatic.indels.vcf --bed {params.target_intervals} --out {output.indel_vcf} --recode --keep-INFO-all
	mv {output.indel_vcf}.recode.vcf {output.indel_vcf}

        NORMAL=`basename {input[0]} | sed 's/.bwamem.final.bam//g'`
	TUMOR=`basename {input[2]} | sed 's/.bwamem.final.bam//g'`
        sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t${{TUMOR}}/g" {output.snv_vcf}
        sed -i "s/FORMAT\\tNORMAL\\tTUMOR/FORMAT\\t${{NORMAL}}\\t${{TUMOR}}/g" {output.indel_vcf}
	
               """

##7	SNVs and Indel Annotation pipeline 

#7a.	VCFfieldEditor.

rule VCFfieldEditor:
	input:
		vcf = "SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/{caller}/{sample}.{aligner}.{base_caller}.raw.vcf",
	output:
		vcf = "SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/{caller}/{sample}.{aligner}.{base_caller}.raw.VAF.vcf"
	version:
		config['VCFfieldEditor']
	params:
		rulename = "VCFfieldEditor",
		VCF_field_script=config["VCFfieldEditor_script"],
		batch	= config["VCFfieldEditor_clust"]
	log:
		"log_error/{sample}.{aligner}.{caller}.VCFfieldEditor.log"
	shell:	"""		
		module load R
		cat {params.VCF_field_script} | R --vanilla --slave --args {wildcards.caller} {input.vcf} 			
	"""

#7b.	SnpEff: Genetic variant annotation and effect prediction toolbox. It annotates and predicts the effects of variants on genes.
rule snpEff:
	input:
		vcf ="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/{caller}/{sample}.{aligner}.{base_caller}.raw.vcf",
	output:
		vcf="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/{caller}/{sample}.{aligner}.{base_caller}.snpEff.vcf",
		txt="SUBJECT/{subject}/"+NOW+"/{sample}/{ref}.{aligner}/{caller}/{sample}.{aligner}.{base_caller}.snpEff.txt"
	version:
		config['snpEff']
	params:
		rulename	="snpEff",
		snpEff_genome	=config["snpEff_genome"],
		snpEff_config	=config["snpEff_config"],
		CosmicCodingMuts=config["resources"]['CosmicCodingMuts'],
		dbsnp		=config["resources"]['dbsnp'],
		clinvar		=config["resources"]['clinvar'],
		ESP_snps	=config["resources"]['ESP_snps'],
		annovar_ver     =config['annovar'],
		vcf2txt_script  =config['vcf2txt_script'],
		batch		=config["snpEff_clust"] 
	log:
		"log_error/{sample}.{aligner}.{caller}.snpEff.vcf.log"
	shell: 	"""
	
	module load snpEff/{version}
	module load java/jre1.7.0_71
	module load annovar/{params.annovar_ver}

 	mkdir /projects/scratch/${{PBS_JOBID}}	
 
	SnpSift_path="/apps/snpEff/{version}/SnpSift.jar"	
        SnpEff_path="/apps/snpEff/{version}/snpEff.jar"
        mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'`

	java -Xmx${{mem}}g -Djava.io.tmpdir=/project/scratch/${{PBS_JOBID}}/ -jar $SnpSift_path dbnsfp -c {params.snpEff_config} -a {input.vcf} | java -Xmx${{mem}}g -jar $SnpSift_path annotate {params.CosmicCodingMuts} - | java -Xmx${{mem}}g -jar $SnpSift_path annotate {params.dbsnp} - | java -Xmx${{mem}}g -jar $SnpSift_path annotate {params.clinvar} - | java -Xmx${{mem}}g -jar $SnpEff_path -t -canon {params.snpEff_genome} > {output.vcf} 2> {log}
	
	perl {params.vcf2txt_script} {output.vcf} /projects/scratch/${{PBS_JOBID}} >{output.txt}
		"""

#7b.	Format VCFS for In-house Annotation pipeline.
rule formatInput:
        input:
                vcfs=lambda wildcards:   ALL_SUBJECT_VCFS[wildcards.subject]
        output:
                "SUBJECT/{subject}/"+NOW+"/Annotation/AnnotationInput.anno",
                "SUBJECT/{subject}/"+NOW+"/Annotation/AnnotationInput.sift"
        params:
                rulename	=	"FormatInput",
		converter	=	config['MakeAnnotationInputs_script'],
		fAEV_script	=	config["fAEV_script"],
                batch		= 	config["formatinput_clust"]
        log:
                "log_error/{subject}.FormatInput"
	
	run:
		inputVCFs = ' '.join(input.vcfs)
                shell("""
                        cut -f 1-5 {inputVCFs} |sort |uniq > SUBJECT/{wildcards.subject}/{NOW}/Annotation/allSites
			touch SUBJECT/{wildcards.subject}/{NOW}/Annotation/AnnotationInput.final.txt
			perl {params.fAEV_script} SUBJECT/{wildcards.subject}/{NOW}/Annotation/allSites SUBJECT/{wildcards.subject}/{NOW}/Annotation/AnnotationInput.final.txt SUBJECT/{wildcards.subject}/{NOW}/Annotation/AnnotationInput.annotations.final.txt SUBJECT/{wildcards.subject}/{NOW}/Annotation/AnnotationInput
                        perl {params.converter} SUBJECT/{wildcards.subject}/{NOW}/Annotation/AnnotationInput 2>{log}
                """)

#7c.	In-house Annotaion Pipeline : Genetic variant annotation using knowledge base databases ( ~60 Sources).
rule Annovar_annotation:
        input:
                "SUBJECT/{subject}/"+NOW+"/Annotation/AnnotationInput.anno"
        output:
                "SUBJECT/{subject}/"+NOW+"/Annotation/AnnotationInput.docm"
        version:
                config['annovar']
        params:
                rulename   	= 	"Annovar_Annotation",
                batch      	= 	config["annovar_clust"],
		TableAnno_script=	config["TableAnno_script"],
		addAnnotation_script=	config["addAnnotation_script"],
		annovar_data	    =	config["annovar_data"]
        log:
                "log_error/{subject}.Annovar"
        shell: """
		module load annovar/{version}
		perl {params.TableAnno_script} SUBJECT/{wildcards.subject}/{NOW}/Annotation/ AnnotationInput {params.addAnnotation_script} {params.annovar_data} 2>> {log}
		"""

#7e.    SIFT :     Annotation tool for annotating coding nonsynonymous SNPs.
rule Sift:
	input:
		sift		="SUBJECT/{subject}/"+NOW+"/Annotation/AnnotationInput.sift",

	output:
		sift_out	="SUBJECT/{subject}/"+NOW+"/Annotation/AnnotationInput.sift.out",
	version:
		config['SIFT'],
	params:
		rulename        ="SIFT",
		build		=config["SIFTbuild"],
		database	=config["SIFT_db"],
		convertor	= config["SiftParse_script"],
		batch           =config["sift_clust"],
	log:
		"log_error/{subject}.SIFT"
	shell: """
		module load python/2.7.9
		module load SIFT/{version}
		
		mkdir /projects/scratch/${{PBS_JOBID}}		
		
		DIR=`pwd`
		cd ${{DIR}}/`dirname {input.sift}`
		FILE=`basename {input.sift}`
		SIFT_exome_nssnvs.pl -i ${{FILE}} -d {params.database} -o /projects/scratch/${{PBS_JOBID}} -z ${{DIR}}/`dirname {input.sift}`/${{FILE}}.sift_predictions.tsv
		perl {params.convertor} ${{DIR}}/`dirname {input.sift}`/${{FILE}}.sift_predictions.tsv > ${{DIR}}/{output.sift_out}
		"""

##8.	PostProcessing of Annotation files.
rule Combine_ASP_annotation:
	input:
		anno			=	"SUBJECT/{subject}/"+NOW+"/Annotation/AnnotationInput.docm",
		sift			=	"SUBJECT/{subject}/"+NOW+"/Annotation/AnnotationInput.sift.out",
                blacklisted		=	config["annovar_data"]+ "hg19_blacklistedSites.txt"
	output:
		anno			=	"SUBJECT/{subject}/"+NOW+"/Annotation/Annotations.coding.rare.txt"
	params:
		rulename		=	"Combine_ASP_annotation",
		AnnotationInput_file	=	config["AnnotationInput_file"],
		dataDir			=	config["annovar_data"],
		convertor		=	config["CombineAnnotations_script"],
		geneanno		=	config["GeneAnnotation_script"],
                filter			=	config["filterVars_script"],
                coding			=	config["proteincoding_script"],
		batch			=	config["combine_clust"]
	log:
		"log_error/{subject}.CombineAnnotation"
	shell: """
		
		for i in `cat {params.AnnotationInput_file}`;do  echo "SUBJECT/{wildcards.subject}/{NOW}/Annotation/$i" >> SUBJECT/{wildcards.subject}/{NOW}/Annotation/list ; done
		sed 's/^[\t ]*//g' -i SUBJECT/{wildcards.subject}/{NOW}/Annotation/list
			
		perl {params.convertor} SUBJECT/{wildcards.subject}/{NOW}/Annotation/list > {output}.tmp
		perl {params.geneanno} {params.dataDir}/hg19_ACMG.txt {output}.tmp >> SUBJECT/{wildcards.subject}/{NOW}/Annotation/AnnotationInput.annotations.final.txt
		perl {params.coding} SUBJECT/{wildcards.subject}/{NOW}/Annotation/AnnotationInput.annotations.final.txt | perl {params.filter} - {input.blacklisted} 0.1 |sort |uniq >{output}
		
		rm -rf {output}.tmp {output}.final.txt 

		"""

rule Attach_ASP_Annotation:
	input:
		txt		=	"SUBJECT/{subject}/"+NOW+"/{sample}/{ref_aligner}/{caller}/{sample}.{aligner}.{base_caller}.snpEff.txt",
		ref		=	"SUBJECT/{subject}/"+NOW+"/Annotation/Annotations.coding.rare.txt",
	output:
		txt		=	"SUBJECT/{subject}/"+NOW+"/{sample}/{ref_aligner}/{caller}/{sample}.{aligner}.{base_caller}.annotated.txt"
	params:
		rulename   	=	"Attach_ASP_Annotation",
		convertor  	= 	config["addBack"],
		batch      	=	config["addbackann_clust"]
	log:
		"log_error/{subject}.AttachAnnotation"
	shell: """
	perl {params.convertor} {input.ref} {input.txt} > {output.txt} 2>{log}
		"""

rule makeDBinput:
	input:
		txtFiles	=	lambda wildcards:	SUBJECT_ANNO[wildcards.subject][wildcards.group],
	output:	
		txt		=	"SUBJECT/{subject}/"+NOW+"/Annotation/db/{subject}.{group}"
	params:
		rulename	=	"makeDBinput",
		convertor	=	config["DBinput_script"],
		batch           =       config["makeDBinput_clust"]
	log:
		"log_error/{subject}.makeDBinput"
	shell: """
	perl {params.convertor} {input.txtFiles} >{output.txt}
	"""

rule Actionable_Somatic:
	input:
		somatic		=	"SUBJECT/{subject}/"+NOW+"/Annotation/db/{subject}.somatic",
		annotation	=	"SUBJECT/{subject}/"+NOW+"/Annotation/Annotations.coding.rare.txt",
		refFile		=	config["annovar_data"]+"hg19_SomaticActionableSites.txt",
		cgc             =       config["annovar_data"]+"geneLists/CancerGeneCensus.v76.txt"
	output:
		somatic		=	"SUBJECT/{subject}/"+NOW+"/Actionable/{subject}.somatic.actionable.txt",
	params:
		rulename	=	"Actionable_Somatic",
		convertor	=	config["Actionable_mutation_script"],
		batch		=	config["Act_somatic_clust"]
	log:
		"log_error/{subject}.Actionable_Somatic"
	shell: """
	perl {params.convertor} somatic  {input.refFile} {input.cgc} {input.somatic} {input.annotation} >{output.somatic}
        """


rule Actionable_Germline:
	input:
		germline	=	"SUBJECT/{subject}/"+NOW+"/Annotation/db/{subject}.germline",
		annotation	=	"SUBJECT/{subject}/"+NOW+"/Annotation/Annotations.coding.rare.txt",
		cancerGeneCensus=	config["annovar_data"]+"geneLists/CGCensus_Hereditary.txt",
		hotspot		=	config["annovar_data"]+"hg19_SomaticActionableSites.txt",
		tsid   		=	config["annovar_data"]+"geneLists/TruSightInheritedDiseases.txt",
		jsw		=	config["annovar_data"]+"geneLists/CancerGenes_JSW.txt",
		clt2		=	config["annovar_data"]+"geneLists/ClinomicsTier2GeneList.txt",
		ghr    		= 	config["annovar_data"]+"geneLists/Genetics_HumanRef.3.8.16.txt"
	output:
		germline	=	"SUBJECT/{subject}/"+NOW+"/Actionable/{subject}.germline.actionable.txt",
	params:
		rulename  	=	"Actionable_Germline",
		convertor	=	config["Actionable_mutation_script"],
		batch    	=	config['Act_germline_clust']
	log:
		"log_error/{subject}.Actionable_Germline"
	shell: """

	if [ -e SUBJECT/{subject}/"+NOW+"/Annotation/db/{subject}.somatic ]
	then
		perl {params.convertor} germline SUBJECT/{subject}/"+NOW+"/Annotation/db/{subject}.somatic {input.germline} {input.annotation} {input.cancerGeneCensus} {input.hotspot} {input.tsid} {input.jsw} {input.clt2} {input.ghr} > {output.germline}
	else
		mkdir -p /projects/scratch/${{PBS_JOBID}}
		touch /projects/scratch/${{PBS_JOBID}}/temp
		perl {params.convertor} germline /projects/scratch/${{PBS_JOBID}}/temp {input.germline} {input.annotation} {input.cancerGeneCensus} {input.hotspot} {input.tsid} {input.jsw} {input.clt2} {input.ghr} > {output.germline}
	fi
        """

rule test1:
	input:
		file="batch_out.txt"
	output:
		file="batch_input.txt"
	log:
		"test.log"
	params:
		batch	=config["snpEff_clust"]
	shell: """
	mem=`echo "{params.batch}" | sed 's/,/\t/g' | sed 's/,/\t/g' | awk '{{split($3,a,"=");print a[2]}}'| sed 's/gb//g'` 
	echo ${{mem}} > {output.file}
		
	"""
