#Load python modules
import itertools
import os
import collections
import pprint
from snakemake.utils import R
from snakemake.exceptions import MissingInputException

# Environment variable for serpentine base directory location
try:
    SERPENTINE_HOME=os.environ['SERPENTINE_HOME']
except KeyError:
    SERPENTINE_HOME="/projects/Clinomics/Tools/serpentine_Tgen/"
    pass

#Basic utility functions
def _get_rule_path(rule_file_path):
    "Combines the serpentine base directory with serpentine rule paths"
    return(os.path.join(SERPENTINE_HOME,rule_file_path))

#Include all config files
configfile: _get_rule_path("cluster.json")
configfile: _get_rule_path("config.json")
configfile: _get_rule_path("version.json")
configfile: _get_rule_path("samplesheet.json")

#Include all rule files
include: _get_rule_path("alignment.rules")

#Store Entity in Dicts
#START####Basic dicts###################################################
UNIT_TO_LIBRARY = {}
for lib,units in config['libraries'].items():
	for unit in units:
	        UNIT_TO_LIBRARY[unit]=lib

LIBRARY_TO_SAMPLE  = {}
for sample,libs in config['samples'].items():
	for lib in libs:
        	LIBRARY_TO_SAMPLE[lib]=sample

SAMPLE_TO_SUBJECT  = {}
for subject,samples in config['subjects'].items():
	for sample in samples:
        	SAMPLE_TO_SUBJECT[sample]=subject

SUBJECT_TO_STUDY  = {}
for study,subjects in config['studies'].items():
	for subject in subjects:
		SUBJECT_TO_STUDY[subject]=study

#END################################################################

#START## Forward one step #########################################
UNIT_TO_SAMPLE = {}
for unit in config['units'].keys():
	UNIT_TO_SAMPLE[unit]=LIBRARY_TO_SAMPLE[UNIT_TO_LIBRARY[unit]]

UNIT_TO_SUBJECT = {}
for unit in config['units'].keys():
    	UNIT_TO_SUBJECT[unit]=SAMPLE_TO_SUBJECT[LIBRARY_TO_SAMPLE[UNIT_TO_LIBRARY[unit]]]

UNIT_TO_STUDY = {}
for unit in config['units'].keys():
    	UNIT_TO_STUDY[unit]=SUBJECT_TO_STUDY[SAMPLE_TO_SUBJECT[LIBRARY_TO_SAMPLE[UNIT_TO_LIBRARY[unit]]]]
#END# ############################################################


#START## BAckward one step ######################################
LIBRARY_TO_UNIT = collections.defaultdict(list)
for unit,sample in UNIT_TO_LIBRARY.items():
        LIBRARY_TO_UNIT[sample].append(unit)

SAMPLE_TO_UNIT = collections.defaultdict(list)
for unit,sample in UNIT_TO_SAMPLE.items():
    	SAMPLE_TO_UNIT[sample].append(unit)

SUBJECT_TO_UNIT = collections.defaultdict(list)
for unit,subject in UNIT_TO_SUBJECT.items():
    	SUBJECT_TO_UNIT[subject].append(unit)

SUBJECT_TO_SAMPLE = collections.defaultdict(list)
for sample,subject in SAMPLE_TO_SUBJECT.items():
        SUBJECT_TO_SAMPLE[subject].append(sample)

STUDY_TO_UNIT = collections.defaultdict(list)
for unit,study in UNIT_TO_STUDY.items():
    	STUDY_TO_UNIT[study].append(unit)

#END ##########################################################

###Making BAM & QC files
ALL_QC_files=[]

BAM_extensions = [".bam",".tdf"]
for qc_ext in BAM_extensions:
	format_string = "SUBJECT/{subject}/{sample}/{reference_name}.{aligner}/bam/{sample}.{aligner}.final"+qc_ext
	file=[format_string.format(subject=SAMPLE_TO_SUBJECT[s], sample=s, reference_name=config['reference_name'], aligner=config['aligner']['bwamem']) for s in config['samples'].keys() ]
	ALL_QC_files.append(file)

QC_metric_extensions = [".bam.depth",".bam.hsmetrics",".bam.hotspot.depth",".bam.flagstat",".bam.qualimapReport.html"]
for qc_ext in QC_metric_extensions:
	if(qc_ext == ".bam.qualimapReport.html"):	
		format_string = "SUBJECT/{subject}/{sample}/{reference_name}.{aligner}/qc/bamqc/{sample}.{aligner}.final"+qc_ext
	else:
		format_string = "SUBJECT/{subject}/{sample}/{reference_name}.{aligner}/qc/{sample}.{aligner}.final"+qc_ext
	file=[format_string.format(subject=SAMPLE_TO_SUBJECT[s], sample=s, reference_name=config['reference_name'], aligner=config['aligner']['bwamem']) for s in config['samples'].keys() ]
	ALL_QC_files.append(file)

ALL_QC_files.append( ["SUBJECT/{subject}/{sample}/{reference_name}.{aligner}/qc/fastqc/{unit}_R1_fastqc.html".format(sample=UNIT_TO_SAMPLE[s], subject=UNIT_TO_SUBJECT[s], unit=s, reference_name=config['reference_name'], aligner=config['aligner']['bwamem']) for s in config['units'].keys()])


###Making pairs for Somatic Callers
somaticPairs = {}
PairsCapture = {}

if len(config['sample_references']) > 0:
	for Tumor in config['sample_references']:
		for Normal in config['sample_references'][Tumor]:
			TumorBam="SUBJECT/{subject}/{sample}/{reference_name}.{aligner}/bam/{sample}.{aligner}.final".format(subject=SAMPLE_TO_SUBJECT[Tumor], sample=Tumor, reference_name=config['reference_name'],aligner=config['aligner']['bwamem'])
			NormalBam =  "SUBJECT/{subject}/{sample}/{reference_name}.{aligner}/bam/{sample}.{aligner}.final".format(subject=SAMPLE_TO_SUBJECT[Normal], sample=Normal, reference_name=config['reference_name'],aligner=config['aligner']['bwamem'])
			PairsCapture[Tumor] = config['sample_captures'][Tumor]
			somaticPairs[Tumor] = [NormalBam + ".bam" , NormalBam + ".bai", TumorBam + ".bam", TumorBam + ".bai"]

#snpEff Annotation
ALL_snpEff_vcf1  = [] ; ALL_snpEff_vcf2  = [] ; ALL_SUBJECT_VCFS = {} 
reference_name=config['reference_name'] ; aligner=config['aligner']['bwamem']
##germline_callers_list = ["haplotypecaller","platypus","freebayes"]
germline_callers_list = ["haplotypecaller","platypus","freebayes","bam2mpg"]
#germline_callers_list = ["bam2mpg"]

for subject in config["subjects"].keys():
	ALL_SUBJECT_VCFS[subject]=[]
	samples = SUBJECT_TO_SAMPLE[subject]
	for sample in samples:
		for caller in germline_callers_list:
			if(caller != "bam2mpg"):
				ALL_SUBJECT_VCFS[subject].append("SUBJECT/"+subject+"/"+sample+"/"+reference_name+"."+aligner+"/"+caller+"/"+sample+"."+aligner+"."+caller+".snpEff.txt")
			else:
				ALL_SUBJECT_VCFS[subject].append("SUBJECT/"+subject+"/"+sample+"/"+reference_name+".novoalign/"+caller+"/"+sample+".novoalign."+caller+".snpEff.txt")
	ALL_snpEff_vcf1.extend(ALL_SUBJECT_VCFS[subject])	

if len(config['sample_references']) > 0:
	for sample in config['sample_references'].keys():
		subject=SAMPLE_TO_SUBJECT[sample]
	        if subject in ALL_SUBJECT_VCFS:
        		ALL_SUBJECT_VCFS[subject].extend([ ("SUBJECT/"+subject+"/"+sample+"/"+reference_name+"."+aligner+"/strelka/"+sample+"."+aligner+".strelka.snvs.snpEff.txt"),
					 		("SUBJECT/"+subject+"/"+sample+"/"+reference_name+"."+aligner+"/strelka/"+sample+"."+aligner+".strelka.indels.snpEff.txt"),
					 		("SUBJECT/"+subject+"/"+sample+"/"+reference_name+"."+aligner+"/mutect/"+sample+"."+aligner+".mutect.snpEff.txt")])
			ALL_snpEff_vcf2.extend(ALL_SUBJECT_VCFS[subject])	
		else:
			print("Seems like there is no Germline caller for "+subject)	

ALL_snpEff_vcf = ALL_snpEff_vcf1 + ALL_snpEff_vcf2
#print(ALL_snpEff_vcf)
#print(ALL_SUBJECT_VCFS)


#Polyphen2 SIFT In-house Annotation
Annot_Annovar	= [] ; Annot_SIFT = [] ; Annot_PPH2 = []
FormatInput_out = ["SUBJECT/{subject}/Annotation/AnnotationInput.anno".format(subject=s) for s in config['subjects'].keys()] + ["SUBJECT/{subject}/Annotation/AnnotationInput.pph".format(subject=s) for s in config['subjects'].keys()] + ["SUBJECT/{subject}/Annotation/AnnotationInput.sift".format(subject=s) for s in config['subjects'].keys()]

#print(FormatInput_out)

Annot_Annovar =["SUBJECT/{subject}/Annotation/AnnotationInput.docm".format(subject=s) for s in config['subjects'].keys()] 
Annot_SIFT = ["SUBJECT/{subject}/Annotation/AnnotationInput.sift".format(subject=s) for s in config['subjects'].keys()] 
Annot_PPH2 = ["SUBJECT/{subject}/Annotation/AnnotationInput.pph".format(subject=s) for s in config['subjects'].keys()] 
Annot_ALL = Annot_Annovar + Annot_SIFT + Annot_PPH2
#print(Annot_SIFT)

CombineAnnotation = [w.replace('docm','annotations.final.txt') for w in Annot_ALL] 

Attach_ASP_Annotation = [w.replace('snpEff', 'annotated') for w in ALL_snpEff_vcf]
#print(Attach_ASP_Annotation)

#test_input = "batch_out.txt"
rule final:
	input:
		ALL_QC_files,
		#ALL_snpEff_vcf,
		#FormatInput_out,
		#Annot_Annovar,
		#CombineAnnotation,
		#Annot_SIFT,
		#Annot_PPH2,
		Attach_ASP_Annotation,
		#Annot_ALL
		#BAMS,
		#test_input
